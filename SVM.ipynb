{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机是一种二分类问题  \n",
    "\n",
    "给定训练样本集  \n",
    "$D = {(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}$  \n",
    "$ y \\in [-1,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM的目标是基于给定的训练样本空间，找到一个划分超平面，将不同类别样本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义这个超平面空间的线性方程：$w_Tx+b=0$  \n",
    "w为平面的法向量，垂直于平面\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定任意点x,可以得到该点到超平面的距离为：  \n",
    "$r =\\frac{|w^Tx-b|}{||w||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义任意超平面的假设函数：\n",
    "$h_w(x) = w^Tx+b$  \n",
    "满足 当$y_i$ = +1时，$w^Tx_i + b > 0$,\n",
    "当$y_i$ = -1时，$w^Tx_i + b < 0$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要保障输入点存在误区的可能性，降低噪音数据的影响，令：  \n",
    "$\\left\\{\n",
    "\\begin{aligned}\n",
    "w^Tx_i + b >= +1&  & if: y_i=+1 \\\\\n",
    "w^Tx_i + b <= -1&  &if: y_i=-1\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "距离超平面最近的几个训练点使以上等式成立，称为“支持向量”，超平面两侧支持向量到超平面的距离之和为  \n",
    "margin = $\\frac{2}{||w||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM的目标是所有的样本点满足以上条件的同时，找到最宽的间隔使得超平面的鲁棒性最好，即对未见实例的泛化能力最强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\max_{w,b} \\frac{2}{||w||}$  \n",
    "s.t. $y_i(w^Tx_i + b) >= 1, i=1,2...n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想要得到最大的margin,就是要最小化||w||,因此:  \n",
    "$min_{w,b} ||w||^2$  \n",
    "s.t. $y_i(w^Tx_i + b) >= 1, i=1,2...n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是向量机的基本型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上分析的都是线性可分模型，对于复杂的非线性分类问题，可以运用核技巧来实现非线性SVM.  \n",
    "复杂的分类边界需要更多的特征值来替换原模型的每一项，这里就可以利用核函数计算出的新特征来拟合这个边界。  \n",
    "以高斯核函数为例：  \n",
    "新的特征值函数\n",
    "$f(x ) = similarity(x_i,l^1) = exp(-\\frac{||x - l^1||^2}{2\\sigma^2})$    \n",
    "其中$||x - l^1||^2 = \\sum_{j=1}{n}(x_j - l_j^1)^2$  \n",
    "$l^1$是预先选定的landmarks地标  \n",
    "这些地标的作用是，如果一个训练实例与地标距离较近，那么新特征的$f_1$值近似于1，  \n",
    "如果x与$l^1$较远，则$f_1$值近似于0。这样可以根据不同的地标的距离关系确定的$w^Tf$来判定x的位置进而判定类别。  \n",
    "一般情况下可以根据训练集的数量来选择地标的数量，即如果训练集有n个实例，则选取n个地标。  \n",
    "[$x^1,x^2,x^3...x^n$]对应[$l^1,l^2,l^3...l^n$]  \n",
    "给定一个训练样本，可以计算x与每个地标的f函数，得到f,计算$w^Tf$> 0 时,预测为y =1,否者反之."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了高斯核函数外，我们还有其他一些选择：  \n",
    "多项式核函数  \n",
    "字符串核函数   \n",
    "卡方核函数  \n",
    "直方图交集核函数等  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于分类问题：一些普遍使用的原则:\n",
    "n为训练样本数，m为特征数\n",
    "1. 如果相较于训练样本数，特征数要大许多，即训练集数据量不够支持我们训练一个非线性的复杂模型，那么一般选取逻辑回归模型或者不带核函数的SVM.\n",
    "2. 如果特征数很少，而且样本数大小中等，比方说，特征数在1-1000之间，而样本数在10-10000之间，可以考虑使用高斯核函数的支持向量机。\n",
    "3. 如果特征值很少，样本值很大，比方m在1-1000之间，而n大于50000，则使用向量机会非常慢，解决方案是创造、增加更多额特征，然后使用逻辑回归或者不带核函数的向量机进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn有对应的模块SVM:\n",
    "from sklearn.svm import LinearSVC,LinearSVR,SVC,SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.svm.LinearSVC(penalty=’l2’, loss=’squared_hinge’, dual=True, tol=0.0001, C=1.0, multi_class=’ovr’, fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
    "```  \n",
    "penalty:惩戒式，l1或者l2  \n",
    "loss:指定损失函数,\"hinge\"or \"squared_hinge\",linearSVC默认是squared_hinge,也就是hinge loss的平方  \n",
    "dual ：选择算法来解决对偶或原始优化问题。当n_samples > n_features 时dual=false。   \n",
    "tol ：（default = 1e - 3）: svm结束标准的精度;   \n",
    "C：目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0；   \n",
    "multi_class：如果y输出类别包含多类，用来确定多类策略， ovr表示一对多，“crammer_singer”优化所有类别的一个共同的目标  \n",
    "如果选择“crammer_singer”，损失、惩罚和优化将会被被忽略。   \n",
    "class_weight ：对于每一个类别i设置惩罚系数C = class_weight[i]*C,如果不给出，权重自动调整为 n_samples / (n_classes * np.bincount(y))   \n",
    "verbose设置运行的时候是否显示详细信息  \n",
    "模型的属性值包括：  \n",
    "support_ 支持向量的索引  \n",
    "support_vectors_ 支持向量  \n",
    "n_support_ 每类的支持向量数  \n",
    "dual_coef_  \n",
    "coef_  \n",
    "intercept_  \n",
    "模型的主要方法包括:  \n",
    "decision_function(x):返回每个样本到划分平面的距离  \n",
    "\n",
    "```\n",
    "SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "```  \n",
    "C：SVC的正则化系数，C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。  \n",
    "C越大，bias越大  \n",
    "kernel：定义特定的核函数，默认为\"rbf\",其他包括：\"linear\",\"poly\",\"sigmoid\",\"precomputed\"  \n",
    "除了上面限定的核函数外，还可以给出自己定义的核函数，其实内部就是用你自己定义的核函数来计算核矩阵。  \n",
    "degree: int型参数 默认为3,这个参数只对多项式核函数有用，是指多项式核函数的阶数n,如果给的核函数参数是其他核函数，则会自动忽略该参数。  \n",
    "gamma: float参数 默认为auto,核函数系数，只对‘rbf’,‘poly’,‘sigmod’有效。如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features.  \n",
    "coef0: float参数 默认为0.0,核函数中的独立项，只有对‘poly’和‘sigmod’核函数有用，是指其中的参数c  \n",
    "probability： bool参数 默认为False,是否启用概率估计。 这必须在调用fit()之前启用，并且会fit()方法速度变慢。  \n",
    "shrinking： bool参数 默认为True,是否采用启发式收缩方式  \n",
    "tol: float参数 默认为1e^-3,svm停止训练的误差精度  \n",
    "cache_size： float参数 默认为200,指定训练所需要的内存，以MB为单位，默认为200MB。  \n",
    "属性值和方法和LinearSVC类似"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
